# Research Gaps and Opportunities in On-Device TinyML Eye-Tracking Systems

## 1. ON-DEVICE TRAINING & LEARNING

### Gap 1.1: Memory-Efficient Backpropagation for Resource-Constrained Devices
**Current State:**
- Traditional backpropagation requires storing all activations (2-3× model size in RAM)
- Most TinyML systems use inference-only models trained in the cloud
- Existing on-device training primarily for simple linear models or small MLPs

**Research Gap:**
- How to perform gradient-based training for CNNs with <512KB RAM?
- Novel algorithms that minimize activation memory without accuracy loss
- Trade-offs between training time, memory, and final model quality

**Research Questions:**
1. Can gradient checkpointing be optimized for specific CNN architectures used in gaze estimation?
2. What is the theoretical minimum memory required for training a given model architecture?
3. How does layer-wise training compare to end-to-end training in accuracy and efficiency?

**Potential Contributions:**
- New backpropagation algorithms specifically designed for embedded systems
- Memory profiling framework for TinyML training
- Guidelines for architecture design that enables efficient on-device training

---

### Gap 1.2: Forward-Forward and Alternative Learning Algorithms for Edge Devices
**Current State:**
- Geoffrey Hinton's Forward-Forward algorithm (2022) eliminates backpropagation
- Limited exploration in TinyML context
- Unclear how well it works for regression tasks (gaze estimation is regression, not classification)

**Research Gap:**
- Effectiveness of backprop-free methods for continuous output tasks (gaze coordinates)
- Convergence speed compared to traditional methods on constrained hardware
- Adaptation strategies for non-classification problems

**Research Questions:**
1. Can Forward-Forward be modified for regression tasks like gaze estimation?
2. What is the accuracy vs. memory trade-off compared to standard backpropagation?
3. Does Forward-Forward enable faster on-device personalization?

**Potential Contributions:**
- First systematic comparison of learning algorithms for TinyML gaze estimation
- Hybrid approaches combining Forward-Forward with lightweight gradient methods
- Benchmark suite for evaluating on-device training algorithms

---

### Gap 1.3: Continual Learning Without Catastrophic Forgetting
**Current State:**
- Continual learning well-studied in cloud/server contexts
- Limited work on embedded continual learning with memory constraints
- Replay buffers are memory-intensive; elastic weight consolidation adds computation

**Research Gap:**
- How to balance plasticity (learning new patterns) and stability (retaining old knowledge) on edge devices?
- Minimal memory replay strategies for embedded systems
- User-specific vs. population-level knowledge retention

**Research Questions:**
1. What is the minimum replay buffer size needed to prevent forgetting in gaze models?
2. Can we predict when catastrophic forgetting will occur based on drift metrics?
3. How do different replay sampling strategies (random vs. strategic) affect performance?

**Potential Contributions:**
- Lightweight continual learning framework for TinyML
- Drift detection metrics optimized for embedded deployment
- Adaptive replay strategies that balance memory and performance

---

### Gap 1.4: Transfer Learning Efficiency for Personalization
**Current State:**
- Pre-train large model → compress → fine-tune is standard
- Unclear which layers to freeze/train for optimal personalization
- Limited study of few-shot learning for gaze estimation on edge devices

**Research Gap:**
- Optimal transfer learning strategies for individual user adaptation with <100 samples
- Which model layers encode person-specific vs. universal gaze features?
- How to automatically determine which layers need personalization?

**Research Questions:**
1. Is there a universal "gaze backbone" that works across all users with only head fine-tuning?
2. How many calibration samples are truly needed for <3° accuracy with proper transfer learning?
3. Can meta-learning (learning to learn) improve few-shot calibration?

**Potential Contributions:**
- Layer-wise analysis of personalization benefits
- Few-shot calibration protocols requiring <5 minutes user time
- Meta-learning framework for rapid user adaptation

---

## 2. MODEL ARCHITECTURE & OPTIMIZATION

### Gap 2.1: Architecture Search for TinyML Gaze Estimation
**Current State:**
- Most models are hand-designed or adapted from image classification architectures
- Neural Architecture Search (NAS) rarely applied to TinyML due to computational cost
- Gaze-specific architectural innovations are limited

**Research Gap:**
- What is the optimal architecture for gaze estimation under strict resource constraints?
- Can we automate architecture discovery for embedded gaze tracking?
- Domain-specific optimizations beyond generic MobileNet/EfficientNet

**Research Questions:**
1. What architectural patterns (depth vs. width, kernel sizes, skip connections) work best for eye images?
2. Can hardware-aware NAS find better architectures than manual design for STM32/ESP32?
3. Are attention mechanisms worth the computational cost for gaze estimation?

**Potential Contributions:**
- First NAS study specifically for TinyML gaze estimation
- Open-source architecture zoo optimized for different hardware platforms
- Design principles for efficient gaze-specific CNNs

---

### Gap 2.2: Quantization-Aware Training for Gaze Regression
**Current State:**
- INT8 quantization standard for classification, but gaze needs continuous outputs
- Unclear how quantization affects angular error (critical metric for gaze)
- Limited work on mixed-precision quantization for hybrid models (gaze + object detection)

**Research Gap:**
- Impact of quantization on regression accuracy (angular error in degrees)
- Optimal bit-widths for different layers in gaze networks
- Quantization strategies that preserve on-device training capability

**Research Questions:**
1. Can we use 4-bit or even binary weights for gaze estimation without sacrificing accuracy?
2. Does quantization-aware training help more for gaze regression than post-training quantization?
3. How to quantize the training process itself (quantized gradients)?

**Potential Contributions:**
- Quantization analysis specifically for angular error metrics
- Mixed-precision strategies for multi-task models (gaze + detection)
- Guidelines for quantizing regression models in TinyML

---

### Gap 2.3: Multi-Task Learning for Integrated Vision Systems
**Current State:**
- Gaze estimation and object detection typically separate models
- Multi-task learning can share features but adds complexity
- Unclear how to balance task priorities on resource-constrained devices

**Research Gap:**
- Can a single unified model perform both gaze estimation and scene understanding efficiently?
- How to design task-specific heads that don't conflict during training?
- Dynamic task scheduling based on user attention and battery level

**Research Questions:**
1. Does multi-task learning reduce total computation compared to separate models?
2. What shared backbone architectures work best for eye images + scene images?
3. Can attention-based routing selectively activate only needed tasks?

**Potential Contributions:**
- Multi-task architecture specifically designed for eye-tracking + scene understanding
- Task scheduling algorithms for battery-aware operation
- Benchmark comparing unified vs. separate model approaches

---

## 3. GAZE-OBJECT FUSION & 3D UNDERSTANDING

### Gap 3.1: Robust Gaze-Object Mapping in Dynamic Scenes
**Current State:**
- Most gaze research uses static scenes or controlled environments
- 2D gaze projection onto scene images has parallax and depth ambiguities
- Limited work on gaze-object correspondence in real-world cluttered scenes

**Research Gap:**
- How to accurately map 2D gaze onto 3D world objects without expensive depth sensors?
- Handling occlusions, moving objects, and scene changes
- Temporal consistency in gaze-object assignments

**Research Questions:**
1. Can monocular depth estimation from scene camera improve gaze-object mapping?
2. How much does head pose estimation from IMU improve 3D gaze accuracy?
3. Can we use saccade patterns and fixations to infer object boundaries?

**Potential Contributions:**
- Probabilistic gaze-object mapping framework accounting for depth uncertainty
- Lightweight 3D scene reconstruction using gaze as a prior
- Evaluation metrics for gaze-object correspondence quality

---

### Gap 3.2: Egocentric Scene Understanding Guided by Gaze
**Current State:**
- Object detection treats all objects equally (no attention mechanism)
- Gaze provides strong signal about object importance and relevance
- Limited work on gaze-conditioned scene understanding

**Research Gap:**
- Can gaze patterns improve object detection accuracy and efficiency?
- Using gaze history to predict future visual attention and pre-load models
- Gaze-based active vision: where to look next for better scene understanding?

**Research Questions:**
1. Does running object detection only in gaze regions reduce computation without losing important objects?
2. Can we train object detectors to prioritize gaze-attended regions?
3. How to model the relationship between gaze patterns and semantic scene content?

**Potential Contributions:**
- Gaze-conditioned object detection for edge devices
- Active vision strategies that optimize information gain per computation
- Dataset of gaze patterns with annotated scene understanding tasks

---

### Gap 3.3: Temporal Modeling of Gaze Behavior
**Current State:**
- Most gaze estimation is frame-by-frame (no temporal context)
- Fixations, saccades, and smooth pursuit have different characteristics
- RNNs/LSTMs are expensive; temporal convolutions underexplored

**Research Gap:**
- How to efficiently model temporal gaze patterns on edge devices?
- Using gaze dynamics (velocity, acceleration) to improve predictions
- Detecting intent from gaze trajectories (e.g., searching vs. reading)

**Research Questions:**
1. Can temporal convolutional networks (TCNs) model gaze dynamics efficiently?
2. Does temporal smoothing improve accuracy more than more complex spatial models?
3. Can we detect user intent (reading, searching, navigating) from gaze patterns alone?

**Potential Contributions:**
- Lightweight temporal models for gaze sequence prediction
- Real-time gaze event detection (fixation, saccade, blink)
- Intent recognition from gaze patterns for proactive assistance

---

## 4. PRIVACY, SECURITY & ETHICS

### Gap 4.1: Privacy-Preserving On-Device Learning
**Current State:**
- On-device training prevents raw data from leaving device (good!)
- But model parameters can leak information about training data
- Federated learning studied for servers, not for wearables

**Research Gap:**
- Can model updates from on-device training leak sensitive information?
- How to enable collaborative learning (multiple users) while preserving privacy?
- Differential privacy mechanisms feasible on resource-constrained devices?

**Research Questions:**
1. What information can be inferred from gaze model weights trained on individual users?
2. Can we quantify privacy leakage from model updates in federated learning scenarios?
3. What is the accuracy-privacy trade-off when adding differential privacy noise?

**Potential Contributions:**
- Privacy analysis of on-device trained gaze models
- Lightweight differential privacy mechanisms for TinyML
- Federated learning protocol for wearable eye-tracking systems

---

### Gap 4.2: Adversarial Robustness of Gaze Estimation
**Current State:**
- Adversarial attacks well-studied for image classification
- Limited work on adversarial robustness of gaze estimation
- Physical adversarial attacks (e.g., printed patterns) unexplored

**Research Gap:**
- Can adversarial perturbations fool gaze estimation models?
- Physical attacks: Can stickers on glasses frames or eye makeup deceive the system?
- Defense mechanisms that work within TinyML constraints

**Research Questions:**
1. How sensitive are gaze models to adversarial perturbations on eye images?
2. Can users intentionally fool the system (privacy concern)?
3. What defenses (adversarial training, input transformations) are practical for edge devices?

**Potential Contributions:**
- First adversarial robustness study for wearable gaze tracking
- Physical attack scenarios and defenses
- Robust model architectures for safety-critical applications

---

### Gap 4.3: Bias and Fairness in Gaze Estimation
**Current State:**
- Most gaze datasets skewed toward certain demographics (white, young, no glasses)
- Performance varies significantly across eye types, skin tones, age groups
- Limited work on fairness-aware model training for gaze

**Research Gap:**
- How to ensure equitable performance across diverse populations?
- Can on-device personalization compensate for dataset bias?
- Measuring and mitigating bias in real-world deployments

**Research Questions:**
1. What is the performance gap across different demographic groups with current models?
2. Does on-device fine-tuning equalize performance or amplify initial bias?
3. Can we train base models that require less personalization for underrepresented groups?

**Potential Contributions:**
- Fairness benchmark for gaze estimation across demographics
- Bias-aware training techniques for more equitable base models
- Guidelines for inclusive dataset collection and model evaluation

---

## 5. POWER & HARDWARE OPTIMIZATION

### Gap 5.1: Ultra-Low-Power Gaze Tracking (<50mW)
**Current State:**
- Current systems consume 1-2W (limits battery life to hours)
- Event-based cameras (DVS) promising but require new algorithms
- Sparse computation not well-explored for gaze estimation

**Research Gap:**
- Can we achieve <50mW average power for continuous gaze tracking?
- Event-based vision for power-efficient eye tracking
- Duty cycling and adaptive frame rate strategies

**Research Questions:**
1. What is the minimum sampling rate for acceptable gaze tracking (currently ~60 FPS)?
2. Can event cameras reduce power by 10× while maintaining accuracy?
3. How to dynamically adjust processing based on gaze stability (fixation vs. saccade)?

**Potential Contributions:**
- Power breakdown analysis of current systems with optimization roadmap
- Event-based gaze estimation algorithms
- Adaptive processing framework for battery-aware operation (target: 24-hour battery life)

---

### Gap 5.2: Hardware-Software Co-Design for Gaze Systems
**Current State:**
- Most research uses off-the-shelf hardware
- Limited exploration of custom accelerators for gaze-specific operations
- Gap between algorithm design and hardware capabilities

**Research Gap:**
- What hardware features would most benefit gaze estimation?
- Custom accelerators for eye image processing (pupil detection, feature extraction)
- FPGA or ASIC implementations for production systems

**Research Questions:**
1. Should eye image preprocessing be done in hardware (custom ISP) or software?
2. What operations dominate computation time and could benefit from acceleration?
3. Can neuromorphic hardware (spiking neural networks) enable ultra-low-power gaze tracking?

**Potential Contributions:**
- Hardware-aware algorithm design for gaze estimation
- Reference design for custom gaze tracking accelerator
- Spiking neural network architectures for event-based gaze tracking

---

### Gap 5.3: Thermal Management and Long-Term Wearability
**Current State:**
- Wearable devices near face have strict temperature limits
- Continuous processing generates heat that affects comfort
- Limited study of thermal dissipation in smart glasses

**Research Gap:**
- How to manage thermal load in near-eye computing?
- Workload scheduling to avoid hotspots
- Materials and form factors for passive cooling

**Research Questions:**
1. What is the maximum acceptable temperature for wearable glasses (skin contact)?
2. How does intermittent processing (duty cycling) affect thermal profile?
3. Can we predict thermal issues and throttle computation proactively?

**Potential Contributions:**
- Thermal characterization of eye-tracking wearables
- Thermally-aware task scheduling algorithms
- Design guidelines for thermal management in smart glasses

---

## 6. HUMAN FACTORS & APPLICATIONS

### Gap 6.1: Calibration-Free or Minimal-Calibration Methods
**Current State:**
- Most systems require 9-16 point calibration (2-5 minutes)
- User burden limits adoption, especially for accessibility applications
- Zero-shot gaze estimation accuracy insufficient (<5-8° error)

**Research Gap:**
- Can we achieve <3° accuracy without explicit calibration?
- Using natural viewing behavior for implicit calibration
- Leveraging population priors with minimal personalization

**Research Questions:**
1. What natural tasks provide enough gaze diversity for implicit calibration (e.g., reading, video watching)?
2. Can we use other modalities (voice commands, hand gestures) as supervision signals?
3. How much accuracy can be gained from each additional calibration point (diminishing returns)?

**Potential Contributions:**
- Implicit calibration framework using unsupervised/self-supervised learning
- Multi-modal calibration combining gaze with other inputs
- Calibration efficiency curves to guide system design

---

### Gap 6.2: Long-Term User Adaptation and Acceptance
**Current State:**
- Most studies are short-term (single session or 1-2 weeks)
- User acceptance of wearables often drops after novelty phase
- Limited understanding of long-term model drift and adaptation

**Research Gap:**
- How do gaze patterns and model performance change over weeks/months?
- What factors predict sustained use vs. abandonment?
- Long-term health effects (eye strain, headaches) under-studied

**Research Questions:**
1. Does model accuracy degrade over weeks without retraining? At what rate?
2. What usage patterns predict long-term adoption vs. abandonment?
3. Are there cumulative effects of daily wearable use (comfort, social acceptance)?

**Potential Contributions:**
- Longitudinal study (6+ months) of wearable eye-tracking usage
- Predictive models of user acceptance and sustained engagement
- Long-term health and comfort guidelines

---

### Gap 6.3: Context-Aware Assistance Strategies
**Current State:**
- Most assistance is reactive (user looks → system responds)
- Limited proactive assistance based on context and behavior patterns
- No personalization of assistance level (everyone gets same help)

**Research Gap:**
- When to provide assistance vs. when to remain silent?
- How to adapt assistance based on user expertise and context?
- Multi-modal output strategies (audio, haptic, visual) optimized for different situations

**Research Questions:**
1. Can we predict when users need help before they explicitly request it?
2. How to avoid "alert fatigue" from over-assisting?
3. What triggers should activate assistance in different contexts (reading vs. navigation vs. social)?

**Potential Contributions:**
- Context-aware assistance framework with user modeling
- Adaptive notification strategies to minimize disruption
- Comparative study of assistance modalities (audio vs. haptic vs. visual)

---

## 7. EVALUATION & BENCHMARKING

### Gap 7.1: Standardized Benchmarks for TinyML Gaze Estimation
**Current State:**
- No standard benchmark suite for embedded gaze tracking
- Papers report different metrics (angular error, precision, latency, power)
- Hard to compare approaches across studies

**Research Gap:**
- Need for standardized evaluation protocol
- Benchmark datasets specifically for on-device training evaluation
- Common hardware platforms for fair comparison

**Research Questions:**
1. What metrics matter most for real-world applications?
2. How to benchmark on-device training efficiency fairly?
3. What test scenarios cover the diversity of real-world usage?

**Potential Contributions:**
- TinyML Gaze Benchmark (dataset, evaluation protocol, leaderboard)
- Reference implementations for common hardware platforms
- Standardized reporting guidelines for research papers

---

### Gap 7.2: Real-World Performance vs. Lab Performance Gap
**Current State:**
- Lab studies show high accuracy (2-3° error) in controlled conditions
- Real-world performance often much worse (5-8° error)
- Limited understanding of what causes degradation

**Research Gap:**
- Systematic analysis of factors that degrade real-world accuracy
- How to close the lab-to-deployment gap?
- Validation metrics that predict real-world performance

**Research Questions:**
1. Which environmental factors most impact accuracy (lighting, head motion, scene clutter)?
2. Can we predict real-world performance from controlled lab tests?
3. What robustness tests should be standard before deployment?

**Potential Contributions:**
- Real-world gaze tracking dataset with ground truth
- Robustness testing protocol for wearable systems
- Domain adaptation techniques to improve field performance

---

### Gap 7.3: User-Centric Evaluation Beyond Accuracy
**Current State:**
- Evaluation focuses on technical metrics (accuracy, latency)
- User experience factors (comfort, social acceptability, trust) under-studied
- Long-term impact on behavior and well-being not measured

**Research Gap:**
- Holistic evaluation framework including technical and human factors
- How do different error types affect user experience? (e.g., 5° error while reading vs. navigating)
- Measuring assistive technology impact on independence and quality of life

**Research Questions:**
1. What level of accuracy is "good enough" for different applications?
2. How do users adapt their behavior to compensate for system errors?
3. What is the relationship between technical performance and perceived usefulness?

**Potential Contributions:**
- Multi-dimensional evaluation framework for assistive wearables
- Application-specific accuracy requirements
- Longitudinal impact study on quality of life for target users

---

## PRIORITY RESEARCH AREAS (High Impact × Feasibility)

### **Tier 1: Foundation (Highest Priority)**
1. **Memory-Efficient On-Device Training Algorithms** - Critical enabler for personalization
2. **Calibration-Free/Minimal Calibration Methods** - Key for user adoption
3. **Standardized TinyML Gaze Benchmark** - Needed for scientific progress
4. **Few-Shot Transfer Learning for Personalization** - Practical impact on usability

### **Tier 2: Optimization (High Value)**
5. **Neural Architecture Search for Gaze Estimation** - Can significantly improve efficiency
6. **Ultra-Low-Power Gaze Tracking** - Essential for all-day wearability
7. **Robust Gaze-Object Mapping in 3D** - Core functionality for applications
8. **Continual Learning Without Forgetting** - Enables long-term use

### **Tier 3: Advanced Applications (Longer-Term)**
9. **Context-Aware Assistance Strategies** - Differentiation for specific use cases
10. **Privacy-Preserving Federated Learning** - Important for multi-user systems
11. **Fairness and Bias Mitigation** - Critical for equitable deployment
12. **Event-Based Vision for Gaze Tracking** - Novel paradigm, high risk/reward

---

## RECOMMENDED RESEARCH FOCUS FOR YOUR PROJECT

Given the scope of an 18-month research project, I recommend focusing on:

**Primary Contribution (Choose 1-2):**
- **On-device training efficiency:** Novel algorithms that enable practical personalization
- **Calibration efficiency:** Minimal-calibration methods using transfer learning

**Secondary Contributions (1-2):**
- **Real-world evaluation:** Bridge lab-to-field gap with systematic study
- **Open benchmark:** Release standardized evaluation suite for community

**Application Validation:**
- Pick one use case (reading assistance OR navigation) and deeply validate
- Longitudinal study showing sustained usage and impact

This focus allows you to:
✓ Make concrete algorithmic contributions (on-device training)
✓ Demonstrate practical impact (minimal calibration, real-world study)
✓ Benefit the research community (benchmark, open-source code)
✓ Generate multiple publications across ML, HCI, and assistive technology venues
